---
title: "deliverable2"
author: "Yanyu Shu"
date: "2/16/2022"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
# Dimensionality Reduction in R
# As the name implies, we want to reduce a set of variables into one (or two) that summarizes them. In this session we will practice two basic techniques:
# Cluster analysis.
# Factor analysis.
```



```{r getData, eval=TRUE}
link='https://raw.githubusercontent.com/ksyanyu/542Final/main/allData.csv'

library(rio)

#getting the data TABLE from the file in the cloud:
fromPy=rio::import(link)
```


```{r}
# Cluster Analysis
# preparing the data
```

```{r}
# I. Data to cluster
```

```{r}
# subset the columns
selection=c("Country", "Percapitaplasticwaste(kg/person/day)", "Plasticwastegeneration(tonnes)", "Mismanagedwaste(percent)", "Mismanagedwaste2025(percent)")
dataToCluster=fromPy[,selection]
```

```{r}
# Set labels as row index
row.names(dataToCluster)=dataToCluster$Country
dataToCluster$Country=NULL
```

```{r}
# Decide if data needs to be transformed:
boxplot(dataToCluster,horizontal = T, las=2,cex.axis=0.4)
```

```{r}
# The data values don't have a similar range, then need to transform the data. Possible 
# alternatives could have been:

# standardizing
as.data.frame(scale(dataToCluster))
```

```{r}
boxplot(scale(dataToCluster),horizontal = T, las=2,cex.axis=0.4)
```

```{r}

scaleDataToCluster=scale(dataToCluster)
boxplot(scaleDataToCluster,horizontal = T, las=2,cex.axis=0.4)
```


```{r}
# II. Compute the DISTANCE MATRIX
```


```{r}
set.seed(999) # this is for replicability of results

# Decide distance method and compute distance matrix:
library(cluster)
scaleDataToCluster_DM=daisy(x=scaleDataToCluster, metric = "gower")
```


```{r}
# replicate the data and dealing with the missing values
dataToCluster2=scaleDataToCluster
# remove all the rows including missing values
dataToCluster3=na.omit(dataToCluster2)
```


```{r}
cor(dataToCluster3)
```



```{r}
dataToCluster3_DM=daisy(x=dataToCluster3, metric = "gower")
```


```{r}
# Compute Clusters
# Computer suggestions
# Using function fviz_nbclust from the library factoextra we can see how many clustered are suggested.
# For partitioning
library(factoextra)
library(ggplot2)
fviz_nbclust(dataToCluster3, 
             pam,
             diss=dataToCluster3_DM,
             method = "gap_stat",
             k.max = 10,verbose = F)
```

```{r}
# for hierarchical (agglomerative)
fviz_nbclust(dataToCluster3, 
             hcut,
             diss=dataToCluster3_DM,
             method = "gap_stat",
             k.max = 10,
             verbose = F,
             hc_func = "agnes")

```

```{r}
# For hierarchical (divisive):
fviz_nbclust(dataToCluster3, 
             hcut,
             diss=dataToCluster3_DM,
             method = "gap_stat",
             k.max = 10,
             verbose = F,
             hc_func = "diana")
```

```{r}
# apply function: indicate a priori the amount of clusters required
NumberOfClusterDesired=3

# Partitioning technique
res.pam = pam(x=dataToCluster3_DM,
              k = NumberOfClusterDesired,
              cluster.only = F)

# Hierarchical technique- agglomerative approach

library(factoextra)
res.agnes= hcut(dataToCluster3_DM, 
                k = NumberOfClusterDesired,
                isdiss=TRUE,
                hc_func='agnes',
                hc_method = "ward.D2")

# Hierarchical technique- divisive approach
res.diana= hcut(dataToCluster3_DM, 
                k = NumberOfClusterDesired,
                isdiss=TRUE,
                hc_func='diana',
                hc_method = "ward.D2")
```

```{r}
# Add results to original data frame:

# the version without missing values
fromPy2=na.omit(fromPy)


fromPy2$pam=as.factor(res.pam$clustering)
fromPy2$agn=as.factor(res.agnes$cluster)
fromPy2$dia=as.factor(res.diana$cluster)
```

```{r}
# Evaluate Results.
# using Plot silhouettes

# from factoextra
fviz_silhouette(res.pam)
```

```{r}
fviz_silhouette(res.agnes)
```

```{r}
library(factoextra)
fviz_silhouette(res.diana)
```

```{r}
# 3.2 Detecting cases badly clustered

# Save individual silhouettes:
# Previos results have saved important information:
head(data.frame(res.pam$silinfo$widths),10)
```

```{r}
pamEval=data.frame(res.pam$silinfo$widths)
agnEval=data.frame(res.agnes$silinfo$widths)
diaEval=data.frame(res.diana$silinfo$widths)

pamPoor=rownames(pamEval[pamEval$sil_width<0,])
agnPoor=rownames(agnEval[agnEval$sil_width<0,])
diaPoor=rownames(diaEval[diaEval$sil_width<0,])
```

```{r}
library("qpcR") 
library(MASS)
library(dplyr)
library(minpack.lm)
library(rgl)
library(robustbase)
library(Matrix)
```
```{r}
bap_Clus=as.data.frame(qpcR:::cbind.na(sort(pamPoor), sort(agnPoor),sort(diaPoor)))
names(bap_Clus)=c("pam","agn","dia")
bap_Clus
```

```{r}
link='https://raw.githubusercontent.com/ksyanyu/542Final/main/allData.csv'

library(rio)

#getting the data TABLE from the file in the cloud:
fromPy=rio::import(link)

# reset indexes to R format:
row.names(fromPy)=NULL
```

```{r}
# dealing with the missing values
# remove all the rows including missing values
dataToRegress=na.omit(fromPy)
```

```{r}
# rename the colomns
names(dataToRegress)=c("Country", "Code", "Region", "Subregion", "Percapitaplasticwaste", "Plasticwastegeneration", "Mismanagedwaste", "Mismanagedwaste2025")
```


```{r}
str(dataToRegress)
```


```{r}
## standardizing
#as.data.frame(scale(dataToRegress2))


column1=dataToRegress[,6]
demoRegress=scale(as.numeric(column1))
```
```{r}
str(demoRegress)
```

```{r}
dataToRegress[,6]=demoRegress
str(dataToRegress)

```

```{r}
# EXPLANATORY APPROACH

# State hypotheses:
# mismanaged plastic waste increase as total plastic waste increase
hypo1=formula(Mismanagedwaste~ Plasticwastegeneration)
```

```{r}
# Compute regression models:
#
# results
gauss1=glm(hypo1,
           data = dataToRegress,
           family = 'gaussian')
```

```{r}
summary(gauss1)
# p<0.001
# significant
```

```{r}
library(rsq)
rsq(gauss1,adj=T)
```

```{r}
plot(gauss1,1)
```

```{r}
# Normality of residuals is assumed:

# Visual exploration:

plot(gauss1,2)

```

```{r}
# The data is normal if the p-value is above 0.05
shapiro.test(gauss1$residuals)
```

```{r}
# Homoscedasticity is assumed, so you need to check if residuals are spread equally along the ranges of predictors:

# Visual exploration:

plot(gauss1, 3)
```

```{r}
library(zoo)
library(lmtest)

#pvalue<0.05 you cannot assume Homoscedasticity
bptest(gauss1) 
```

```{r}
# cannot assume homoscedasticity
# residuals are not spread equally along the ranges of predictors, 
# in other word, the variance of the dependent variable is the same for all the data.
```

```{r}
# a summary plot of the work:
library(sjPlot)
## Learn more about sjPlot with 'browseVignettes("sjPlot")'.
plot_models(gauss1,vline.color = "grey")
```


